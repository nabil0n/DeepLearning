{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20e December\n",
    "\n",
    "# LLM finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_hub'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "if not os.path.exists('imdb_sentiment.keras'):\n",
    "    classifier = keras_hub.models.BertClassifier(\n",
    "        'bert_base_en_uncased',\n",
    "        activation='softmax',\n",
    "        num_classes=2,\n",
    "    )\n",
    "    imdb_train, imdb_test = tfds.load(\n",
    "        'imdb_reviews',\n",
    "        split=['train', 'test'],\n",
    "        as_supervised=True,\n",
    "        batch_size=16,\n",
    "    )\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\"imdb_sentiment.keras\", save_best_only=True)]\n",
    "    classifier.fit(\n",
    "        imdb_train,\n",
    "        validation_data=imdb_test,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "classifier = keras.models.load_model('imdb_sentiment.keras')\n",
    "preds = classifier.predict(\"\"\"\n",
    "\n",
    "Unfairly, this has been judged a lesser entry in the National Lampoon stable, though itís arguably only second to the original vacation and proves that Chevy Chase was once funny. Clarke (Chase) does his best to orchestrate a traditional happy family Christmas, but is thwarted by odious relatives and plain old bad luck.\n",
    "\n",
    "If it doesn't make you at least giggle, then you clearly don't understand the true meaning of the festive season - which is of course combustible toilets and electrified cats.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "\n",
    "- Low Rank Adaption\n",
    "- A parameter-efficient fine-tuning technique for LLMs.\n",
    "- Kan låsa parameterar som vi ändå inte använder.\n",
    "- Förtränade modeller är ofta \"över parametereiserade\" för vår önskade användning.\n",
    "- Så även då vi sätter in ytterligare parametrar så sänks den totala mängden parametrar!\n",
    "- Fixed rank matrisen (kommer ingen förklaring då det är lite överkurs)\n",
    "- Reducerar mängden GPU minne som krävs\n",
    "- Tränar modellen snabbare (i vissa fall)\n",
    "- INGEN EXTRA LATENCY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copypasta från pytorch tutorial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    in_dim: int,\n",
    "    out_dim: int,\n",
    "    rank: int,\n",
    "    alpha: float,\n",
    "    dropout: float\n",
    "  ):\n",
    "    # These are the weights from the original pretrained model\n",
    "    self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "\n",
    "    # These are the new LoRA params. In general rank << in_dim, out_dim\n",
    "    self.lora_a = nn.Linear(in_dim, rank, bias=False)\n",
    "    self.lora_b = nn.Linear(rank, out_dim, bias=False)\n",
    "\n",
    "    # Rank and alpha are commonly-tuned hyperparameters\n",
    "    self.rank = rank\n",
    "    self.alpha = alpha\n",
    "\n",
    "    # Most implementations also include some dropout\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    # The original params are frozen, and only LoRA params are trainable.\n",
    "    self.linear.weight.requires_grad = False\n",
    "    self.lora_a.weight.requires_grad = True\n",
    "    self.lora_b.weight.requires_grad = True\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # This would be the output of the original model\n",
    "    frozen_out = self.linear(x)\n",
    "\n",
    "    # lora_a projects inputs down to the much smaller self.rank,\n",
    "    # then lora_b projects back up to the output dimension\n",
    "    lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n",
    "\n",
    "    # Finally, scale by the alpha parameter (normalized by rank)\n",
    "    # and add to the original model's outputs\n",
    "    return frozen_out + (self.alpha / self.rank) * lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\n",
    "\n",
    "# Build Llama2 without any LoRA layers\n",
    "base_model = llama2_7b()\n",
    "\n",
    "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
    "# We just need to define which layers we want LoRA applied to.\n",
    "# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n",
    "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
    "# layers outside of the self-attention.\n",
    "lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
